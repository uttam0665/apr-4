{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nbformat as nbf\n",
    "\n",
    "# Create a Jupyter Notebook\n",
    "nb = nbf.v4.new_notebook()\n",
    "\n",
    "# Add a title cell\n",
    "nb.cells.append(nbf.v4.new_markdown_cell(\"# Decision Tree Assignment\"))\n",
    "\n",
    "# Add an introduction cell\n",
    "nb.cells.append(nbf.v4.new_markdown_cell(\"## Decision Tree-1\"))\n",
    "\n",
    "# Q1: Decision Tree Classifier Algorithm\n",
    "nb.cells.append(nbf.v4.new_markdown_cell(\"### Q1: Describe the decision tree classifier algorithm and how it works to make predictions.\"))\n",
    "nb.cells.append(nbf.v4.new_markdown_cell(\"\"\"\n",
    "A decision tree classifier is a supervised learning algorithm used for classification tasks. It works by splitting the dataset into subsets based on the value of input features. This process is repeated recursively, forming a tree structure where each node represents a feature, each branch represents a decision rule, and each leaf node represents an outcome.\n",
    "\n",
    "To make predictions, the decision tree algorithm follows the tree structure from the root node to a leaf node, applying the decision rules at each node to determine the class label of a given input sample.\n",
    "\"\"\"))\n",
    "\n",
    "# Q2: Mathematical Intuition Behind Decision Tree Classification\n",
    "nb.cells.append(nbf.v4.new_markdown_cell(\"### Q2: Provide a step-by-step explanation of the mathematical intuition behind decision tree classification.\"))\n",
    "nb.cells.append(nbf.v4.new_markdown_cell(\"\"\"\n",
    "The mathematical intuition behind decision tree classification involves the following steps:\n",
    "\n",
    "1. **Calculate Entropy**: Entropy measures the impurity or randomness in the dataset. It is given by:\n",
    "    \\[ H(S) = - \\sum_{i=1}^{c} p_i \\log_2(p_i) \\]\n",
    "   where \\( p_i \\) is the probability of class \\( i \\) in the dataset \\( S \\).\n",
    "\n",
    "2. **Calculate Information Gain**: Information gain measures the reduction in entropy after splitting the dataset based on a feature. It is given by:\n",
    "    \\[ IG(S, A) = H(S) - \\sum_{v \\in \\text{Values}(A)} \\frac{|S_v|}{|S|} H(S_v) \\]\n",
    "   where \\( S_v \\) is the subset of \\( S \\) for which feature \\( A \\) has value \\( v \\).\n",
    "\n",
    "3. **Select the Best Feature**: The feature with the highest information gain is selected for splitting the dataset.\n",
    "\n",
    "4. **Repeat**: Repeat the process recursively for each subset until the stopping criteria are met (e.g., maximum depth, minimum samples per leaf).\n",
    "\"\"\"))\n",
    "\n",
    "# Q3: Decision Tree for Binary Classification\n",
    "nb.cells.append(nbf.v4.new_markdown_cell(\"### Q3: Explain how a decision tree classifier can be used to solve a binary classification problem.\"))\n",
    "nb.cells.append(nbf.v4.new_markdown_cell(\"\"\"\n",
    "A decision tree classifier can be used to solve a binary classification problem by splitting the dataset based on the values of input features to separate the two classes. The tree structure is formed by recursively applying decision rules at each node, with the goal of maximizing the separation between the two classes at each split. The leaf nodes represent the class labels (e.g., 0 or 1) for the binary classification problem.\n",
    "\"\"\"))\n",
    "\n",
    "# Q4: Geometric Intuition Behind Decision Tree Classification\n",
    "nb.cells.append(nbf.v4.new_markdown_cell(\"### Q4: Discuss the geometric intuition behind decision tree classification and how it can be used to make predictions.\"))\n",
    "nb.cells.append(nbf.v4.new_markdown_cell(\"\"\"\n",
    "The geometric intuition behind decision tree classification is that it divides the feature space into rectangular regions, where each region corresponds to a class label. Each decision rule at a node represents a boundary that splits the feature space based on the value of a feature. By following the decision rules from the root node to a leaf node, the decision tree assigns a class label to a given input sample based on which region of the feature space it falls into.\n",
    "\"\"\"))\n",
    "\n",
    "# Q5: Confusion Matrix\n",
    "nb.cells.append(nbf.v4.new_markdown_cell(\"### Q5: Define the confusion matrix and describe how it can be used to evaluate the performance of a classification model.\"))\n",
    "nb.cells.append(nbf.v4.new_markdown_cell(\"\"\"\n",
    "A confusion matrix is a table that summarizes the performance of a classification model by showing the number of true positive (TP), true negative (TN), false positive (FP), and false negative (FN) predictions. It can be used to evaluate the performance of a classification model by calculating metrics such as accuracy, precision, recall, and F1 score.\n",
    "\"\"\"))\n",
    "\n",
    "# Q6: Example of Confusion Matrix\n",
    "nb.cells.append(nbf.v4.new_markdown_cell(\"### Q6: Provide an example of a confusion matrix and explain how precision, recall, and F1 score can be calculated from it.\"))\n",
    "nb.cells.append(nbf.v4.new_code_cell(\"\"\"\n",
    "# Example of a confusion matrix\n",
    "y_true = [0, 0, 1, 1, 0, 1, 0, 1, 1, 0]\n",
    "y_pred = [0, 1, 1, 1, 0, 0, 0, 1, 1, 0]\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(\"Confusion Matrix:\\\\n\", cm)\n",
    "\n",
    "# Calculating precision, recall, and F1 score\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "\"\"\"))\n",
    "\n",
    "# Q7: Importance of Choosing an Appropriate Evaluation Metric\n",
    "nb.cells.append(nbf.v4.new_markdown_cell(\"### Q7: Discuss the importance of choosing an appropriate evaluation metric for a classification problem and explain how this can be done.\"))\n",
    "nb.cells.append(nbf.v4.new_markdown_cell(\"\"\"\n",
    "Choosing an appropriate evaluation metric for a classification problem is important because it determines how the performance of the model is measured and compared. Different metrics focus on different aspects of model performance, such as precision (accuracy of positive predictions), recall (ability to identify positive instances), and F1 score (harmonic mean of precision and recall). The choice of metric should align with the specific goals and requirements of the classification problem.\n",
    "\n",
    "For example, in medical diagnosis, recall may be more important to minimize false negatives, while in spam detection, precision may be more important to minimize false positives.\n",
    "\"\"\"))\n",
    "\n",
    "# Q8: Example Where Precision is the Most Important Metric\n",
    "nb.cells.append(nbf.v4.new_markdown_cell(\"### Q8: Provide an example of a classification problem where precision is the most important metric, and explain why.\"))\n",
    "nb.cells.append(nbf.v4.new_markdown_cell(\"\"\"\n",
    "An example where precision is the most important metric is spam email detection. In this case, it is crucial to minimize false positives (i.e., legitimate emails incorrectly classified as spam) because such errors can result in important emails being missed by the user. By focusing on precision, the model ensures that the emails classified as spam are truly spam, reducing the likelihood of legitimate emails being marked as spam.\n",
    "\"\"\"))\n",
    "\n",
    "# Q9: Example Where Recall is the Most Important Metric\n",
    "nb.cells.append(nbf.v4.new_markdown_cell(\"### Q9: Provide an example of a classification problem where recall is the most important metric, and explain why.\"))\n",
    "nb.cells.append(nbf.v4.new_markdown_cell(\"\"\"\n",
    "An example where recall is the most important metric is medical diagnosis for a serious condition (e.g., cancer detection). In this case, it is crucial to minimize false negatives (i.e., patients with the condition incorrectly classified as healthy) because such errors can result in missed diagnoses and delayed treatment. By focusing on recall, the model ensures that most patients with the condition are correctly identified, even if it means some healthy patients are incorrectly classified as having the condition.\n",
    "\"\"\"))\n",
    "\n",
    "# Save the notebook\n",
    "with open(\"/mnt/data/Decision_Tree_Assignment.ipynb\", \"w\") as f:\n",
    "    nbf.write(nb, f)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
